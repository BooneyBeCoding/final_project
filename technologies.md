TECHNOLOGIES and METHODOLOGIES LIST:
====================================
1. Kaggle - datasets
    -Kaggle had assorted post, timing, and user data for each of the primary 3 stock-market-finance-oriented subreddits we wanted to look at
2. Jupyter - virtual environment
    -launching Jupyter notebooks through Anaconda virtual environments is the most familiar setup the team has for managing and working and refactoring data
3. Github - version management
    -being able to independently work on branches ourselves and merge file sets into one larger place, on top of having a place to push projects or files that are still in-process is an incredibly valuable tool 
    -Github housing this project will also enable us to link our progress and individual contributions as proof to potential future employers as a practical applied example of what we learned in this course
4. Python - scripting language
    -the various libraries of python and packages accompanying them are the ones the team is most familiar with; working, refactoring, reorienting, and creating visualizations from the dataset through this scripting language is convenient given the expansive tools that already exist for it
5. Quickbase App - ERD
    -we aren't really drafting a relational or comparative multi-dataset model with our quesiton as is, since the data from the 3 main sources is already compiled together; nevertheless understanding data types and having this resource makes the model more widely applicable outside of this original use
6. PostgreSQL - database housing/data storage and management
    -our data is largely structured as we found it, and creating refactored or tailored down tables for visualizations is easy with this application
7. Vader - Python package for Sentiment Analysis
    -https://towardsdatascience.com/sentimental-analysis-using-vader-a3415fef7664
    -the above package is what we largely be applying to our existing dataset to (hopefully) provide discernible and quantifiable difference between the 3 main sources for our dataset--this package is already fully built out for Python, as well
8. Tableau - visualization
    -for graphs, word maps, and other visualizations, we chose Tableau as it is one of the major technologies for presentation valued in the professional environment and broadening our familiarity with it is both a useful tool for the team members as well as being marketable to potential future employers
9. VS Code Basic - file creation, idea structure, and organization
    -markdown, structuring and planning, and other drafting instances are easy to manage through this program/text editor
10. Pandas - data cleaning
    -formatting and applying uniform transformation to the mostly structured but still somewhat raw data in our kaggle set needs to be done, and this is what we chose to do that with, given our present levels of exposure
11. SciKitLearn - Machine Learning/logistic regression
    -SKLearn is really the resource we think ought to be applied for our Machine Learning model, given that our plan is to run logistic regression over our refactored results and data to answer the origional Research Question / Proposal
